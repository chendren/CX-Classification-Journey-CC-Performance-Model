# Mistral 7B Training Configuration for Mac (M4 Max)
# Optimized for Apple Silicon with 64GB RAM

model:
  name: "mistralai/Mistral-7B-v0.1"  # Using pre-downloaded v0.1 model
  max_seq_length: 2048
  quantization: "none"  # Using 16-bit - faster and better quality with 64GB RAM

training:
  output_dir: "../models/mistral-7b-contact-center"
  num_train_epochs: 3

  # Batch sizes optimized for M4 Max with 64GB RAM
  per_device_train_batch_size: 2  # Conservative for memory
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size: 16

  # Learning rate and optimization
  learning_rate: 2.0e-5
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"

  # Logging and checkpointing
  logging_steps: 10
  eval_steps: 250  # Less frequent on Mac
  save_steps: 500
  save_total_limit: 2  # Keep only 2 checkpoints to save disk space
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"

  # Mac-specific optimizations
  fp16: true  # Use 16-bit precision for speed
  bf16: false  # MPS works with fp16
  gradient_checkpointing: true  # Essential for memory efficiency
  dataloader_num_workers: 0  # Mac MPS works best with single worker

lora:  # Parameter-efficient fine-tuning
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

data:
  train_file: "data/train_temporal.jsonl"
  validation_file: "data/validation_temporal.jsonl"
  test_file: "data/test_temporal.jsonl"
  max_length: 2048

# No wandb for local training
wandb:
  enabled: false
